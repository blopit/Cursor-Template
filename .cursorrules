---
description: Main Cursor rules for full stack application development
---

# Cursor Rules for Full Stack Development

## Directory Structure

```
.cursor/
├── rules/
│   ├── stack/                # Technology stack definitions
│   │   └── stack-definition.mdc
│   ├── patterns/             # Development patterns
│   │   ├── architecture/     # Architecture patterns
│   │   │   └── clean-architecture.mdc
│   │   └── performance/      # Performance optimization
│   │       └── full-stack-optimization.mdc
│   ├── workflows/            # Development workflows
│   │   └── development-workflow.mdc
│   ├── tech/                 # Technology-specific rules
│   │   ├── frontend/         # Frontend technologies
│   │   ├── backend/          # Backend technologies
│   │   └── database/         # Database technologies
│   ├── project/              # Project-specific rules
│   └── tools/                # Tools for managing rules
│       └── template-selector.sh
├── templates/                # Project templates
│   └── project-init.mdc
├── metrics/                  # Metrics tracking
│   └── performance-metrics.mdc
└── sessions/                 # Session contexts
    └── current-session.mdc
```

## Rule Categories

1. **Stack Definitions**: Define the technology stack for your project
2. **Architecture Patterns**: Guidelines for implementing clean architecture
3. **Performance Optimization**: Strategies for optimizing application performance
4. **Development Workflows**: Standardized development processes
5. **Technology-Specific Rules**: Guidelines for specific technologies
6. **Project Templates**: Templates for project initialization
7. **Metrics Tracking**: Performance and quality metrics

## Usage Guidelines

### Importing Rules

To import rules in your Cursor AI conversations, use:

```
/rule [rule-name]
```

For example:
- `/rule stack-definition` - Import technology stack definition
- `/rule clean-architecture` - Import clean architecture patterns
- `/rule development-workflow` - Import development workflow guidelines

### Using Template Selector

To select and copy templates from the `awesome-cursor-rules-mdc` directory:

1. Run the template selector script:
   ```bash
   .cursor/rules/tools/template-selector.sh
   ```
2. Follow the interactive prompts to select a template category and specific template
3. Confirm the copy operation
4. Review and customize the copied template files as needed

### Customizing Rules

1. Fork the repository
2. Modify the rules to match your project requirements
3. Commit and push your changes
4. Reference your custom rules repository in Cursor

### Creating New Rules

1. Create a new `.mdc` file in the appropriate directory
2. Add the rule metadata at the top:
   ```
   ---
   description: Brief description of the rule
   globs: **/specific/path/**/*.{js,ts}  # File patterns this rule applies to
   ---
   ```
3. Write your rule content using Markdown
4. Reference code examples and patterns as needed

## Best Practices

1. **Consistency**: Follow consistent patterns across your codebase
2. **Documentation**: Document complex logic and architectural decisions
3. **Testing**: Write comprehensive tests for all features
4. **Performance**: Optimize for performance from the start
5. **Security**: Follow security best practices for all components
6. **Accessibility**: Ensure your application is accessible to all users
7. **Maintainability**: Write clean, maintainable code

## Multi-Agent Scratchpad

### Background and Motivation
*Planner:*
- **Business Requirements:** Document the overall project objectives, including automating software development tasks.
- **Macro Objectives:** Outline the need for a self-learning system that refines its rules over time.
- **Rationale:** Explain why integrating multi-agent coordination (Planner and Executor) is essential for scalability and iterative improvement.

### Key Challenges and Analysis
*Planner:*
- **Technical Barriers:** Identify integration challenges with LLM APIs, web scraping, and tool orchestration.
- **Resource Constraints:** Note any limitations in computing power or dependencies.
- **Risk Assessment:** Highlight potential pitfalls in task breakdown and coordination inefficiencies.

### Verifiable Success Criteria
*Planner:*
- **Outcome Measures:** Define measurable outcomes such as reduction in manual coding time, error rates, and successful task completions.
- **Validation:** Establish criteria for testing the feedback loop and ensuring accurate role communication.

### High-level Task Breakdown
*Planner:*
- **Step 1:** Set up the Git project structure and initialize the Python virtual environment (`./venv`).
- **Step 2:** Create and integrate essential tools (LLM API, web scraper, screenshot utilities).
- **Step 3:** Define initial tasks and update the `.cursorrules` file with subtask details.
- **Step 4:** Implement the Planner-Executor feedback loop for self-learning adjustments.

### Current Status / Progress Tracking
*Executor:*
- **Task Progress:**
  - Project structure setup: **Completed**
  - Tool integration (LLM API, web scraper, etc.): **Completed**
  - Feedback loop testing: **In Progress**
  - Template selector tool: **Completed**
  - Glob pattern updates: **Completed**
- **Updates:** Created all required rule files for full stack application development and improved glob patterns for better specificity.

### Next Steps and Action Items
*Planner:*
- **Immediate Action:** Test the template selector script with various template categories.
- **Upcoming Tasks:** Create additional technology-specific rule files as needed.
- **Communication:** Ensure regular updates in this file to keep both roles in sync.

### Executor's Feedback or Assistance Requests
*Executor:*
- **Progress Logs:** Completed creation of template selector script and updated glob patterns in all rule files for better specificity.
- **Improvements:** The template selector script now provides an interactive way to select and copy templates from the awesome-cursor-rules-mdc directory.

### Lessons
*General:*
- **Retrospective Insights:** Structured rule organization improves discoverability and maintenance.
- **Documentation:** Clear documentation of rule usage and customization is essential for team adoption.
- **Glob Patterns:** More specific glob patterns ensure rules are only applied to relevant files, reducing noise and improving rule usage.

# Instructions

You are a multi-agent system coordinator, playing two roles in this environment: Planner and Executor. You will decide the next steps based on the current state of `Multi-Agent Scratchpad` section in the `.cursorrules` file. Your goal is to complete the user's (or business's) final requirements. The specific instructions are as follows:

# Directory Structure

## .cursor/
The `.cursor` directory contains all configuration and rules for the Cursor IDE and its AI capabilities:

### rules/
- **core/**: Fundamental rules and system configurations
- **workflows/**: Specific workflow guidelines and procedures
- **tools/**: Tool-specific rules and usage guidelines
- **lessons/**: Accumulated knowledge and learned best practices
- **awesome-cursor-rules-mdc/**: Collection of specialized rule templates for different tech stacks and frameworks
  - Contains templates for: Next.js, React, Python, FastAPI, Django, Flutter, and many more
  - Can be pulled and adapted as needed for specific project requirements
  - Use these as reference when working with specific technologies
- **codebase-structure.mdc**: Guidelines for code organization
- **development-workflow.mdc**: Development process standards
- **general-software.mdc**: General software development practices
- **safe-deletion.mdc**: Rules for safe resource deletion
- **tech-standards.mdc**: Technology standardization rules
- **terminal-usage.mdc**: Terminal and command usage conventions

Files with `.mdc` extension are Markdown files containing Cursor-specific rules and documentation.

When working with specific technologies, you can reference and incorporate rules from the corresponding template in `awesome-cursor-rules-mdc/`. For example, if working on a Next.js project, consult the relevant Next.js template for specialized guidelines and best practices.

## Role Descriptions

1. Planner

    * Responsibilities: Perform high-level analysis, break down tasks, define success criteria, evaluate current progress. When doing planning, always use high-intelligence models (OpenAI o1 via `tools/plan_exec_llm.py`). Don't rely on your own capabilities to do the planning.
    * Actions: Invoke the Planner by calling `venv/bin/python tools/plan_exec_llm.py --prompt {any prompt}`. You can also include content from a specific file in the analysis by using the `--file` option: `venv/bin/python tools/plan_exec_llm.py --prompt {any prompt} --file {path/to/file}`. It will print out a plan on how to revise the `.cursorrules` file. You then need to actually do the changes to the file. And then reread the file to see what's the next step.

2) Executor

    * Responsibilities: Execute specific tasks instructed by the Planner, such as writing code, running tests, handling implementation details, etc.. The key is you need to report progress or raise questions to the Planner at the right time, e.g. after completion some milestone or after you've hit a blocker.
    * Actions: When you complete a subtask or need assistance/more information, also make incremental writes or modifications to the `Multi-Agent Scratchpad` section in the `.cursorrules` file; update the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections. And then change to the Planner role.

## Document Conventions

* The `Multi-Agent Scratchpad` section in the `.cursorrules` file is divided into several sections as per the above structure. Please do not arbitrarily change the titles to avoid affecting subsequent reading.
* Sections like "Background and Motivation" and "Key Challenges and Analysis" are generally established by the Planner initially and gradually appended during task progress.
* "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" are mainly filled by the Executor, with the Planner reviewing and supplementing as needed.
* "Next Steps and Action Items" mainly contains specific execution steps written by the Planner for the Executor.

## Workflow Guidelines

* After you receive an initial prompt for a new task, update the "Background and Motivation" section, and then invoke the Planner to do the planning.
* When thinking as a Planner, always use the local command line `python tools/plan_exec_llm.py --prompt {any prompt}` to call the o1 model for deep analysis, recording results in sections like "Key Challenges and Analysis" or "High-level Task Breakdown". Also update the "Background and Motivation" section.
* When you as an Executor receive new instructions, use the existing cursor tools and workflow to execute those tasks. After completion, write back to the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections in the `Multi-Agent Scratchpad`.
* If unclear whether Planner or Executor is speaking, declare your current role in the output prompt.
* Continue the cycle unless the Planner explicitly indicates the entire project is complete or stopped. Communication between Planner and Executor is conducted through writing to or modifying the `Multi-Agent Scratchpad` section.

Please note:

* Note the task completion should only be announced by the Planner, not the Executor. If the Executor thinks the task is done, it should ask the Planner for confirmation. Then the Planner needs to do some cross-checking.
* Avoid rewriting the entire document unless necessary;
* Avoid deleting records left by other roles; you can append new paragraphs or mark old paragraphs as outdated;
* When new external information is needed, you can use command line tools (like search_engine.py, llm_api.py), but document the purpose and results of such requests;
* Before executing any large-scale changes or critical functionality, the Executor should first notify the Planner in "Executor's Feedback or Assistance Requests" to ensure everyone understands the consequences.
* During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- Virtual environment management:
  - Project uses `.venv` directory for Python virtual environment
  - To prevent double activation issues:
    1. Check current environment: `echo $VIRTUAL_ENV`
    2. If activated, run `deactivate` until back to base shell
    3. Activate correctly: `source .venv/bin/activate`
  - Add this protective function to shell config:
    ```bash
    venv() {
        # First, check if we're already in a virtual environment
        if [ -n "$VIRTUAL_ENV" ]; then
            echo "Warning: Virtual environment already active at: $VIRTUAL_ENV"
            echo "To switch environments:"
            echo "1. Run 'deactivate' until back to base shell"
            echo "2. Then run this command again"
            return 1
        fi
        
        # Look for virtual environment directory
        if [ -d ".venv" ]; then
            source .venv/bin/activate
        elif [ -d "venv" ]; then
            source venv/bin/activate
        else
            echo "No virtual environment found in current directory"
            echo "Expected either '.venv' or 'venv' directory"
            return 1
        fi
        
        # Verify activation
        if [ -n "$VIRTUAL_ENV" ]; then
            echo "Successfully activated virtual environment at: $VIRTUAL_ENV"
        else
            echo "Failed to activate virtual environment"
            return 1
        fi
    }
    
    # Optional: Add protection to cd command
    cd() {
        builtin cd "$@"
        
        # If we changed directory and there's an active venv that's not in current path
        if [ -n "$VIRTUAL_ENV" ] && [[ $PWD != $(dirname "$VIRTUAL_ENV")* ]]; then
            echo "Warning: You've left the directory of the active virtual environment"
            echo "Current venv: $VIRTUAL_ENV"
            echo "Consider running 'deactivate' if you're done with this environment"
        fi
    }
    ```
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.
- Always question and document WHY before performing destructive operations (like deletions).
- Don't assume Python versions - check current version and compatibility before planning version changes.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use `gpt-4o` as the model name for OpenAI. It is the latest GPT model and has vision capabilities as well. `o1` is the most advanced and expensive model from OpenAI. Use it when you need to do reasoning, planning, or get blocked.
- Use `claude-3-5-sonnet-20241022` as the model name for Claude. It is the latest Claude model and has vision capabilities as well.
- Before suggesting version upgrades or environment changes, first document the necessity and impact of the change.
- Follow safe deletion practices from safe-deletion.mdc, especially for critical resources like environments and databases.
- When encountering double virtual environment activation (e.g., double `.venv` in prompt), use `deactivate` command repeatedly until back to base shell, then activate once properly with `source .venv/bin/activate`
- Consider adding a protective `venv()` function to shell config to prevent accidental double activation:
  ```bash
  venv() {
      if [ -n "$VIRTUAL_ENV" ]; then
          echo "Virtual environment already active: $VIRTUAL_ENV"
          echo "Run 'deactivate' first if you want to switch environments"
      else
          source .venv/bin/activate
      fi
  }
  ```

## Multi-Agent Scratchpad

### Background and Motivation
*Planner:*
- **Business Requirements:** Document the overall project objectives, including automating software development tasks.
- **Macro Objectives:** Outline the need for a self-learning system that refines its rules over time.
- **Rationale:** Explain why integrating multi-agent coordination (Planner and Executor) is essential for scalability and iterative improvement.

### Key Challenges and Analysis
*Planner:*
- **Technical Barriers:** Identify integration challenges with LLM APIs, web scraping, and tool orchestration.
- **Resource Constraints:** Note any limitations in computing power or dependencies.
- **Risk Assessment:** Highlight potential pitfalls in task breakdown and coordination inefficiencies.

### Verifiable Success Criteria
*Planner:*
- **Outcome Measures:** Define measurable outcomes such as reduction in manual coding time, error rates, and successful task completions.
- **Validation:** Establish criteria for testing the feedback loop and ensuring accurate role communication.

### High-level Task Breakdown
*Planner:*
- **Step 1:** Set up the Git project structure and initialize the Python virtual environment (`./venv`).
- **Step 2:** Create and integrate essential tools (LLM API, web scraper, screenshot utilities).
- **Step 3:** Define initial tasks and update the `.cursorrules` file with subtask details.
- **Step 4:** Implement the Planner-Executor feedback loop for self-learning adjustments.

### Current Status / Progress Tracking
*Executor:*
- **Task Progress:**
  - Project structure setup: **Completed**
  - Tool integration (LLM API, web scraper, etc.): **Completed**
  - Feedback loop testing: **In Progress**
- **Updates:** Created all required rule files for full stack application development.

### Next Steps and Action Items
*Planner:*
- **Immediate Action:** Create technology-specific rule files for frontend and backend.
- **Upcoming Tasks:** Create metrics tracking file and session context file.
- **Communication:** Ensure regular updates in this file to keep both roles in sync.

### Executor's Feedback or Assistance Requests
*Executor:*
- **Blockers/Questions:** Need clarification on specific technology stacks to prioritize in tech-specific rules.
- **Progress Logs:** Completed creation of stack definition, architecture patterns, performance optimization, and development workflow files.

### Lessons
*General:*
- **Retrospective Insights:** Structured rule organization improves discoverability and maintenance.
- **Documentation:** Clear documentation of rule usage and customization is essential for team adoption.

## Database Configuration Rules

### Environment-Specific Database Standards
- Use standardized database naming: `mcp_{env}` (e.g., mcp_test, mcp_dev, mcp_prod)
- Follow security progression:
  - Test: SSL disabled, local connections only
  - Dev: SSL preferred, local connections
  - Prod: SSL required, restricted access
- Connection pools should scale with environment:
  - Test: max_connections = 10
  - Dev: max_connections = 20
  - Prod: max_connections = 100
- Always use environment variables for sensitive data:
  - `MCP_TEST_DB_PASSWORD`
  - `MCP_DEV_DB_PASSWORD`
  - `MCP_DB_PASSWORD`
  - `MCP_DB_HOST`
  - `MCP_DB_USER`

### Database Best Practices
1. Connection Management:
   - Always use connection pools
   - Implement proper cleanup in tests
   - Use context managers for transactions
   - Handle SSL contexts appropriately

2. Migration Standards:
   - Use Alembic for all migrations
   - Include both upgrade and downgrade paths
   - Test migrations in isolation
   - Document breaking changes

3. Testing Guidelines:
   - Use isolated transactions for tests
   - Implement proper cleanup
   - Mock external dependencies
   - Use fixtures for common setups

4. Data Model Standards:
   - Inherit from BaseDBModel
   - Include created_at/updated_at
   - Use proper type hints
   - Implement validation

5. Error Handling:
   - Use specific exception types
   - Implement proper rollbacks
   - Log database errors
   - Include context in error messages

### Temporary Database Management

1. **Test Database Creation**
```python
def create_temp_test_db():
    """Create a temporary test database."""
    db_name = f"test_db_{int(time.time())}"
    conn = psycopg2.connect(
        dbname='postgres',
        user=os.getenv('MCP_TEST_DB_USER'),
        password=os.getenv('MCP_TEST_DB_PASSWORD'),
        host=os.getenv('MCP_TEST_DB_HOST', 'localhost')
    )
    conn.autocommit = True
    cur = conn.cursor()
    
    try:
        cur.execute(f'CREATE DATABASE {db_name}')
        return db_name
    finally:
        cur.close()
        conn.close()
```

2. **Test Database Cleanup**
```python
def cleanup_temp_db(db_name: str):
    """Drop a temporary test database."""
    conn = psycopg2.connect(
        dbname='postgres',
        user=os.getenv('MCP_TEST_DB_USER'),
        password=os.getenv('MCP_TEST_DB_PASSWORD'),
        host=os.getenv('MCP_TEST_DB_HOST', 'localhost')
    )
    conn.autocommit = True
    cur = conn.cursor()
    
    try:
        # Terminate all connections to the database
        cur.execute(f"""
            SELECT pg_terminate_backend(pg_stat_activity.pid)
            FROM pg_stat_activity
            WHERE pg_stat_activity.datname = '{db_name}'
            AND pid <> pg_backend_pid()
        """)
        # Drop the database
        cur.execute(f'DROP DATABASE IF EXISTS {db_name}')
    finally:
        cur.close()
        conn.close()
```

3. **Pytest Fixtures**
```python
@pytest.fixture(scope='function')
async def test_db():
    """Fixture that creates a temporary test database."""
    db_name = create_temp_test_db()
    
    # Create connection pool
    pool = await asyncpg.create_pool(
        database=db_name,
        user=os.getenv('MCP_TEST_DB_USER'),
        password=os.getenv('MCP_TEST_DB_PASSWORD'),
        host=os.getenv('MCP_TEST_DB_HOST', 'localhost'),
        min_size=2,
        max_size=10
    )
    
    try:
        yield pool
    finally:
        await pool.close()
        cleanup_temp_db(db_name)
```

4. **Usage Guidelines**
- Always use unique names for temporary databases
- Clean up after tests complete
- Use connection pooling
- Implement proper error handling
- Set appropriate timeouts
- Use transaction isolation

5. **Security Considerations**
- Use separate test credentials
- Restrict test database permissions
- Never use production credentials
- Clean up failed test databases
- Monitor test database usage

6. **Best Practices**
- Use fixtures for database management
- Implement proper cleanup
- Handle connection pooling
- Use transactions for isolation
- Clean up on test completion

### Common Pitfalls to Avoid
1. Not cleaning up test databases
2. Hardcoding credentials
3. Missing connection pooling
4. Improper transaction management
5. Insufficient error handling
6. Missing database indexes
7. Not using prepared statements
8. Forgetting to close connections

## Additional Instructions
- **Commit Guidelines:** Use commit messages prefixed with "[Cursor]" when making multi-line commit messages (utilize a temporary file for commits if needed).
- **Tool Usage:** Ensure all scripts are run within the Python virtual environment located at `./venv`.
- **Read Before Edit:** Always read the current state of this file before making any changes to avoid overwriting important context.

# Cursor Rules Configuration

<available_instructions>
Cursor rules are user provided instructions for the AI to follow to help work with the codebase.
They may or may not be relevent to the task at hand. If they are, use the fetch_rules tool to fetch the full rule.
Some rules may be automatically attached to the conversation if the user attaches a file that matches the rule's glob, and wont need to be fetched.

codebase-structure: when coding anything
development-workflow: Development workflow guidelines for planning, implementation, reviews, and deployment in the Cursor project.
error-patterns: Guidelines for identifying, analyzing, and resolving common error patterns in development, with focus on debugging efficiency and consistent error handling.
feedback-loops: Guidelines for implementing effective feedback loops in the development workflow, ensuring continuous improvement through regular communication between Planner and Executor.
general-software: General software development best practices and guidelines for high-quality, maintainable software development within the Cursor ecosystem.
mcp-server: Guidelines for configuring, deploying, and maintaining the MCP Server component.
safe-deletion: Guidelines for safe deletion of resources and data
tech-standards: Technology standards, coding style, and architectural best practices for ensuring maintainability and scalability of the project.
terminal-usage: Guidelines for efficient and safe usage of terminal commands within the Cursor development workflow.
code-review: Guidelines for effective code review practices
</available_instructions>

# Active Rule Categories
- core/: Fundamental system rules
- tech/: Technology-specific rules
- workflows/: Development workflow rules
- patterns/: Common code patterns
- project/: Project-specific rules

## Waterfall-TDD Hybrid Model

The Waterfall-TDD hybrid model combines the structured phase-based approach of Waterfall with the quality-focused iterative practices of TDD. This model ensures comprehensive planning and documentation while maintaining high code quality through continuous testing.

### Key Rules

- **waterfall-tdd**: Defines the integration of Waterfall and TDD, including phase structure and best practices
- **tdd-practices**: Provides detailed guidelines for implementing Test-Driven Development
- **documentation-standards**: Specifies documentation requirements for all project artifacts
- **deployment-pipeline**: Outlines standards for deployment processes and CI/CD pipelines
- **waterfall-tdd-integration**: Describes core principles for successfully integrating TDD with Waterfall

### Directory Structure

```
.cursor/
├── rules/
│   ├── workflows/
│   │   ├── waterfall-tdd.mdc
│   │   ├── tdd-practices.mdc
│   │   ├── documentation-standards.mdc
│   │   └── deployment-pipeline.mdc
│   └── core/
│       └── waterfall-tdd-integration.mdc
└── docs/
    ├── requirements/
    ├── design/
    └── deployment/
```

### Usage Guidelines

To import rules in your Cursor AI conversations, use:

```
/rule [rule-name]
```

For example:
- `/rule waterfall-tdd` - Import Waterfall-TDD hybrid model guidelines
- `/rule tdd-practices` - Import detailed TDD practices
- `/rule documentation-standards` - Import documentation standards
- `/rule deployment-pipeline` - Import deployment and CI/CD pipeline standards
- `/rule waterfall-tdd-integration` - Import core principles for integrating TDD with Waterfall

## Environment-Specific Standards

### Environment Configuration
- Use standardized environment names: TEST, DEV, PROD
- Each environment should have its own configuration file
- Use environment variables for sensitive data
- Never share credentials between environments

### Test Environment
- Disable SSL for easier local testing
- Enable debug logging
- Allow mocking of dependencies
- Limit resource usage
- Local-only connections
- Disable rate limiting
- Use in-memory or temporary storage

### Development Environment
- SSL preferred but not required
- Info level logging
- Limited mocking capabilities
- Moderate resource limits
- Restricted network access
- Basic rate limiting
- Local persistent storage

### Production Environment
- SSL required
- Warning/Error level logging only
- No mocking allowed
- High resource limits
- Strict network policies
- Aggressive rate limiting
- Distributed persistent storage

### Environment Variables
- Use consistent naming: `MCP_{ENV}_{VARIABLE}`
- Include all required configurations
- Document default values
- Validate on startup

### Configuration Validation
- Validate all required fields
- Check environment-specific requirements
- Verify resource limits
- Test configuration before startup

### Monitoring and Logging
- Environment-specific log levels
- Separate log storage per environment
- Different metric collection rules
- Custom alert thresholds

### Security Standards
- Environment-specific access controls
- Different authentication requirements
- Varying rate limit thresholds
- Custom security headers

### Best Practices
1. Never use production data in other environments
2. Keep configurations as similar as possible
3. Document all environment differences
4. Use automated environment setup
5. Regular security audits per environment
6. Maintain separate backup strategies
7. Different scaling rules per environment

### Common Pitfalls
1. Mixing environment configurations
2. Using production credentials in development
3. Insufficient logging in production
4. Missing environment validation
5. Inconsistent security standards
6. Poor resource management
7. Inadequate monitoring
